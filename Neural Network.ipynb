{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f021dd",
   "metadata": {},
   "source": [
    "# Розробока нейронної мережі без використання бібліотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d85dfe",
   "metadata": {},
   "source": [
    "Цей проєкт створено для поглибленного ознайомлення із алгоритмом роботи нейронних мереж. Для розробки використовуються суто структури даних Python та Numpy (бібліотека з функціями для лінійної алгебри)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "bcbc1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from micrograd.engine import Value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4e4c75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createValue(x):\n",
    "    return x if type(x) == Value else Value(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f3c57a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=1, grad=0)\n",
      "Value(data=1, grad=0)\n"
     ]
    }
   ],
   "source": [
    "print(createValue(1))\n",
    "print(createValue(Value(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d770fb",
   "metadata": {},
   "source": [
    "## Функція активації"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84b663",
   "metadata": {},
   "source": [
    "Функція активації використовується для використання __нелінійності__ при побудові функції класифікації (або регресії).\n",
    "\n",
    "Найбільш поширеними функціями активації є:\n",
    "- Sigmoid\n",
    "- Tanh\n",
    "- RelU\n",
    "- LeakyRelU\n",
    "\n",
    "Для даної нейронної мережі буде використана функція RelU:\n",
    "\n",
    "RelU = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e13ae",
   "metadata": {},
   "source": [
    "## RelU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4735bd",
   "metadata": {},
   "source": [
    "Функція активації RelU може бути використана з бібліотеки micrograd для легкого обчислення похідної складної функції\n",
    "\n",
    "RelU = `max(0, x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28102fee",
   "metadata": {},
   "source": [
    "## Нейрон"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd990c31",
   "metadata": {},
   "source": [
    "Виступає атомарним будівельник блоком нейронної мережі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f5485e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        self.inputs = list(map(lambda i: createValue(i), inputs))\n",
    "        self.weights = list(map(lambda w: createValue(w), weights))\n",
    "        self.bias = createValue(bias)\n",
    "        self.value = None\n",
    "        \n",
    "    def output(self):\n",
    "        result = createValue(0)\n",
    "        for input, weight in zip(self.inputs, self.weights):\n",
    "            result += input * weight\n",
    "        result += self.bias\n",
    "        return result\n",
    "    \n",
    "    def set_inputs(self, inputs):\n",
    "        # TODO akicha: використання функціональності через композицію обʼєкту\n",
    "        self.inputs = list(map(lambda i: createValue(i), inputs))\n",
    "        \n",
    "    def forward(self):\n",
    "        # функція активації імпортована з мікроград бібліотеки для правильного прорахунку\n",
    "        # похідної складної функції під час зворотньої пропагації\n",
    "        self.value = NeuralNetwork.activation(self.output())\n",
    "    \n",
    "    def backward(self):\n",
    "        # На момент виконання, .backward обовʼязкового повинен бути виконаний на функції втрат (loss)\n",
    "        for idx, weight in self.weights:\n",
    "            dLdwidx = self.weights[idx].grad\n",
    "            self.weights[idx] -= NeuralNetwork.learning_rate * dLdwidx\n",
    "                \n",
    "        dLdb = self.bias.grad\n",
    "        self.bias -= NeuralNetwork.learning_rate * dLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9a2858f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputNeuron:\n",
    "    def __init__(self, v):\n",
    "        self.value = createValue(v)\n",
    "    \n",
    "    # noop\n",
    "    def forward():\n",
    "        pass\n",
    "    \n",
    "    # noop\n",
    "    def backward():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9f90dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, prevLayer, numNeurons):\n",
    "        self.neurons = []\n",
    "        self.prevLayer = prevLayer;\n",
    "        inputs = prevLayer.values();\n",
    "        for neuron in range(numNeurons):\n",
    "            # при ініціалізації ми не знаємо правильних значень ваги тому ініціалізуємо випадково\n",
    "            # пізніше, нейронна мережа сама знайде правильні значення\n",
    "            weights = np.random.randn((len(inputs)))\n",
    "            # при ініціалізації ми не знаємо правильного значення байесу\n",
    "            # пізніше, нейронна мережа сама знайде правильне значення\n",
    "            bias = np.random.randn()\n",
    "            self.neurons.append(Neuron(inputs, weights, bias))\n",
    "        \n",
    "    def forward(self):\n",
    "        inputs = self.prevLayer.values();\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            neuron.set_inputs(inputs)\n",
    "            neuron.forward()\n",
    "    \n",
    "    def backward(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.backward()\n",
    "            \n",
    "    # TODO akicha: використання методу через композицію обʼєктів\n",
    "    def values(self):\n",
    "        return list(map(lambda n: n.value, self.neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "414ceacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self, inputs):\n",
    "        self.neurons = []\n",
    "        for i in range(len(inputs)):\n",
    "            self.neurons.append(InputNeuron(inputs[i]))\n",
    "    \n",
    "    # дані нейрони можна і потрібно перезаписувати так як кожен нейрон - це характеристика обʼєкту\n",
    "    # що змінюється із кожною ітерацією\n",
    "    def set_inputs(self, inputs):\n",
    "        self.neurons = []\n",
    "        for i in range(len(inputs)):\n",
    "            self.neurons.append(InputNeuron(inputs[i]))\n",
    "    \n",
    "    # noop\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    # noop\n",
    "    def backward(self):\n",
    "        pass\n",
    "            \n",
    "    def values(self):\n",
    "        return list(map(lambda n: n.value, self.neurons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e93fa",
   "metadata": {},
   "source": [
    "## Нейронна мережа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9ed33d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO akicha: перемістити дані в окремий клас\n",
    "loss_plot_x = []\n",
    "loss_plot_y = []\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        return x.relu()\n",
    "        \n",
    "    # y - очікуєме значення\n",
    "    # y_hat - спрогнозоване значення\n",
    "    def loss(self, y):\n",
    "        outputLayer = self.layers[-1]\n",
    "        # N = 1 так як тільки один нейрон присутній в останньому шарі нейронної мережі\n",
    "        N = len(outputLayer.values())\n",
    "        # на даний момент в нейронній мережі є лише один нейрон у останньому шарі нейронної мережі\n",
    "        y_hat = self.prediction()\n",
    "        return createValue(1) / createValue(N) * (createValue(y) - y_hat)**2\n",
    "    \n",
    "    def forward(self):\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "            \n",
    "    def backward(self):\n",
    "        for layer in range(len(self.layers)):\n",
    "            print(f'backward at {-idx - 1}')\n",
    "            layer = self.layers[-idx - 1]\n",
    "            layer.backward()\n",
    "            \n",
    "    def prediction(self):\n",
    "        outputLayer = self.layers[-1]\n",
    "        return outputLayer.values()[0]\n",
    "    \n",
    "    # TODO akicha: create new class from data\n",
    "    def gradient(self, epochs, data):\n",
    "        steps = 0\n",
    "        for e in epochs:\n",
    "            print(f'start epoch {e}')\n",
    "            \n",
    "            for data_sample in data:\n",
    "                expected_value = data_sample[2]\n",
    "                # шар вхідних даних\n",
    "                self.layers[0].set_values([data_sample[0], data_sample[1]])\n",
    "            \n",
    "                # логування початкових значень нейронів у кожному з шарів\n",
    "                for idx, layer in self.layers:\n",
    "                    print(self.layers[idx].values(), f'the values {idx} before forward')\n",
    "\n",
    "                self.forward()\n",
    "\n",
    "                # логування значень після forward кроку нейронів у кожному з шарів\n",
    "                for idx, layer in self.layers:\n",
    "                    print(self.layers[idx].values(), f'the values {idx} after forward')\n",
    "\n",
    "                y_hat = self.prediction()\n",
    "                print(y_hat, 'the prediction')\n",
    "\n",
    "                loss = self.loss(expected_value)\n",
    "\n",
    "                # micrograd\n",
    "                loss.backward()\n",
    "                \n",
    "                self.backward()\n",
    "                \n",
    "                steps += 1\n",
    "                \n",
    "                loss_plot_x.append(steps)\n",
    "                loss_plot_y.append(loss.data)\n",
    "    \n",
    "    learning_rate = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8403977",
   "metadata": {},
   "source": [
    "## Набір даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c0c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [length, width, class]\n",
    "entities_class_1 = [\n",
    "    [3, 1, 1],\n",
    "    [2, 2, 1]\n",
    "]\n",
    "\n",
    "entities_class_2 = [\n",
    "    [1, 3, 0],\n",
    "    [1.5, 2.5, 1]\n",
    "]\n",
    "\n",
    "# набір даних для тестування\n",
    "entities = entities_class_1 + entities_class_2\n",
    "\n",
    "# набір даних для тесту\n",
    "entity_secret = [1.75, 2.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a13bd",
   "metadata": {},
   "source": [
    "## Пісочниця для тестування проміжного результату роботи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "aad61e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=1, grad=0), Value(data=2, grad=0)]\n",
      "[Value(data=0, grad=0), Value(data=1.1721227845820006, grad=0)]\n",
      "[Value(data=1.7969188664138014, grad=0)]\n",
      "Value(data=1.7969188664138014, grad=0)\n",
      "0\n",
      "-1.4824232236921713\n"
     ]
    }
   ],
   "source": [
    "inputLayer = InputLayer([])\n",
    "hiddenLayer = Layer(inputLayer, 2)\n",
    "outputLayer = Layer(hiddenLayer, 1)\n",
    "n = NeuralNetwork([inputLayer, hiddenLayer, outputLayer])\n",
    "\n",
    "n.gradient(10, entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6ff93",
   "metadata": {},
   "source": [
    "## Графік залежності функції втрат від кількості кроків тренування моделі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot_x, loss_plot_y)\n",
    "\n",
    "plt.xlabel('Ітерація')\n",
    "plt.ylabel('Похибка')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91dedd",
   "metadata": {},
   "source": [
    "## Візуалізація набору даних"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f3e94",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(map(lambda e: e[0], entities_class_1))\n",
    "y = list(map(lambda e: e[1], entities_class_1))\n",
    "\n",
    "x2 = list(map(lambda e: e[0], entities_class_2))\n",
    "y2 = list(map(lambda e: e[1], entities_class_2))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x2, y2)\n",
    "plt.scatter(entity_secret[0], entity_secret[1])\n",
    "\n",
    "\n",
    "plt.legend(['тип 1', 'тип 2', 'невідомий тип'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209f4af",
   "metadata": {},
   "source": [
    "## Тренування моделі"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72237726",
   "metadata": {},
   "source": [
    "### Ініціалізація"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501b86c",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "neuron = Neuron([], [0.2, 0.7], 0)\n",
    "\n",
    "plot_data = []\n",
    "\n",
    "for i in range(1, 300):\n",
    "    random_entity = random.choice(entities)\n",
    "    \n",
    "    neuron.set_inputs([random_entity[0], random_entity[1]])\n",
    "\n",
    "    # print(f'Random entity is of type: {random_entity[2]}')\n",
    "\n",
    "    # print(f'Iteration number: {i}')\n",
    "\n",
    "    output = neuron.forward()\n",
    "    # print(f'Predicted type is: {output}')\n",
    "    \n",
    "    loss = neuron.loss(random_entity[2], output)\n",
    "    if (i % 10 == 0):\n",
    "        print(f'Calculated Loss: {loss}')\n",
    "\n",
    "    \n",
    "    neuron.backward(random_entity[2], output)\n",
    "    \n",
    "    plot_data.append([i, loss])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
